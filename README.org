#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:{} arch:headline author:t broken-links:nil
#+OPTIONS: c:nil creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+OPTIONS: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t title:t toc:t
#+OPTIONS: todo:t |:t
#+TITLE: README
#+DATE: <2017-12-08 Fri>
#+AUTHOR: Gregory Grubbs
#+EMAIL: gregoryg@cloudera.com
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 25.3.1 (Org mode 9.1.3)
#+SETUPFILE: https://raw.githubusercontent.com/fniessen/org-html-themes/master/setup/theme-readtheorg.setup

* Overview

  Datasets and Demos by Cloud and Infrastructure SE Specialization

  Bring up clusters with Cloudera Director.  The Director template for AWS is [[file:director/aws/cdsw/aws-ohio-cdsw-dev.conf][here]].  This
  template includes the configuration needed to activate Hue Notebooks.

  Modify the config for your own usage in the =provider= section: =region=, =keyName=,
  =subnetId=, =securityGroupsIds=, etc.  

** Hints and tips

   When using the Hue notebooks, bring the notebook up in Hue, then make a copy using Save
   As...  This resets the internal IDs and makes the notebook usable.

   When using the Tableau workbooks, edit the workbook (XML format) and change the server
   info in the data source before opening the workbook.

* Scripts to run on gateway node

Scripts reside in the [[file:ingest/][ingest]] directory

**** Set up the basics of the environment with [[file:ingest/01-setup-environment.sh][ingest/01-setup-environment.sh]]
   + Installs useful software, including =pip=, =jq=, =git=
   + Sets up user HDFS directory, and adds =data= subdirectory
   + Sets up =.impalarc= and =.beeline/beeline.properties= for easy shell access to Hive
     and Impala
**** Load Hive/Impala external S3 tables and optimized tables with [[file:ingest/02-load-all-demos.sh][ingest/02-load-all-demos.sh]]
   + External tables are created for the "raw" data access to original datasets.  These
     datasets reside on AWS S3
   + Impala queries then create tables optimized for performance stored as Parquet on
     HDFS.
**** Set up Connected Car demo with [[file:ingest/03-setup-connected-car.sh][ingest/03-setup-connected-car.sh]]
     This incomplete script compiles the Scala code used with the Connected Car demo.

* Datasets loaded by the above scripts

** [[Retail Data Journey][Retail Data Journey]]

   Dataset is loaded into the Hive/Impala =retailer= database

   [[file:notebooks/hue/Data Journey - Retail.json][Data Journey - Retail]] Hue notebook

   [[file:notebooks/tableau/Data Journey Retail.twb][Data Journey - Retail]] Tableau workbook

** [[BABS - Bay Area Bikeshare]]

   This demo follows this blog post: [[http://gethue.com/bay-area-bike-share-data-analysis-with-spark-notebook-part-2/][Bay Area bike share analysis with the Hadoop Notebook and Spark & SQL]]

   Dataset is loaded into the =bikeshare= database

   Raw data:  s3a://gregoryg/babs

   [[file:notebooks/hue/Bay Area BikeShare.json][Bay Area BikeShare Hue notebook]]


** BCHC - Big Cities Health Coalition
   [[https://bchi.bigcitieshealth.org/indicators/1867/13503][Full data set]] source website

   Converted from .csv to .tsv using Excel (or Pandas)
   
   [[file:notebooks/hue/Big Cities Health Analysis.json][BCHC Hue Notebook]]

   [[file:notebooks/tableau/Big Cities Health Coalition.twb][BCHC Tableau Workbook]]

** NYC Taxi and Uber

** Airlines dataset

** Utility data

** Hive benchmark dataset with User visits and website rankings
   



