## Config with CDSW node and Cloud9 dev node
 # Cluster name
environmentName: "c-137"
deploymentName: teenyverse
name: quantum-carburetor

#
# Cloud provider configuration
provider {
  type: aws
  #
  # Get AWS credentials from the OS environment
  # See http://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html
  #
  # If specifying the access keys directly and not through variables, make sure to enclose
  # them in double quotes.
  #
  # Not needed when running on an instance launched with an IAM role.

  accessKeyId: ${?AWS_ACCESS_KEY_ID}
  secretAccessKey: ${?AWS_SECRET_ACCESS_KEY}

  #
  # ID of the Amazon AWS region to use
  # See: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html
  region: us-east-2 # Ohio

  #
  # Region endpoint (if you are using one of the Gov. regions)
  # regionEndpoint: ec2.us-gov-west-1.amazonaws.com

  # keyName refers to a key for the AWS user in the region
  keyName: gregoryg-ohio

  #
  # ID of the VPC subnet
  # See: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html
  #

  subnetId: subnet-e903ae92 # 172.16.20.0/24 in us-east-2

  #
  # Comma separated list of security group IDs
  # See: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html
  #
  # Default security group

  securityGroupsIds: sg-bce9bfd5

  #
  # Specify a size for the root volume (in GBs). Cloudera Director will automatically expand the
  # filesystem so that you can use all the available disk space for your application
  # See: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage_expand_partition.html
  #

  rootVolumeSizeGB: 200 # defaults to 50 GB if not specified

  #
  # Specify the type of the EBS volume used for the root partition. Defaults to gp2
  # See: http://aws.amazon.com/ebs/details/
  #

  rootVolumeType: gp2 # 

  #
  # Whether to associate a public IP address with instances or not. If this is false
  # we expect instances to be able to access the internet using a NAT instance
  #
  # Currently the only way to get optimal S3 data transfer performance is to assign
  # public IP addresses to your instances and not use NAT instances (public subnet setup)
  #
  # See: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-ip-addressing.html
  #

  associatePublicIpAddresses: true

}

#
# SSH credentials to use to connect to the instances
#
ssh {
  #     # username: ec2-user # for RHEL image
  username: centos
  privateKey: ${HOME}/.ssh/gregoryg-ohio.pem # absolute path to .pem file
  # keyName: gregoryg-ncalifornia
}

#
# A list of instance types to use for group of nodes or management services
#
instances {
  t2m  {
    type: t2.medium
    image: ami-b37e5dd6 # simple basic Centos 7.3
    tags {owner: gregoryg}
    normalizeInstance: true
    bootstrapScriptsPaths: ["java8-bootstrap.sh"]
  }
  m42x {
    type: m4.2xlarge   # requires an HVM AMI
    # image: ami-24eccb41 # pre-baked 5.11.0 CM/CDH
    # image: ami-bf5878da # pre-baked 5.12
    image: ami-b37e5dd6 # simple basic Centos 7.3
    # iamProfileName: iam-profile-REPLACE-ME
    tags {owner: gregoryg}
    normalizeInstance: true
    bootstrapScriptsPaths: ["java8-bootstrap.sh"]
  }

  m42x-disks {
    type: m4.2xlarge   # requires an HVM AMI
    # image: ami-24eccb41 # pre-baked 5.11.0 CM/CDH
    # image: ami-bf5878da # pre-baked 5.12
    image: ami-b37e5dd6 # simple basic Centos 7.3
    ebsVolumeCount: 3
    ebsVolumeType: st1
    ebsVolumeSizeGiB: 1000
    tags {owner: gregoryg}
    normalizeInstance: true
    bootstrapScriptsPaths: ["java8-bootstrap.sh"]
  }
}

#
# Optional external database server configuration.
#
# Cloudera Director can create databases on existing database servers or
# provision RDS database servers on-demand.
#
databaseServers {
  mysqlmeta {
    type: mysql
    host: 172.16.20.227
    port: 3306
    user: cmdbadmin
    password: cmdbadmin
  }
}

#
# Configuration for Cloudera Manager. Cloudera Director can use an existing Cloudera Manager
# or bootstrap everything from scratch for a new cluster
#
cloudera-manager {
  # password: cloudera
  count: 1
  instance: ${instances.m42x} {
    instanceNamePrefix: cm
    tags {
      application: "Cloudera Manager 5"
    }
  }
  javaInstallationStrategy: NONE
  repository: "http://archive.cloudera.com/cm5/redhat/7/x86_64/cm/5.13/"
  repositoryKeyUrl: "http://archive.cloudera.com/cm5/redhat/7/x86_64/cm/RPM-GPG-KEY-cloudera"
  csds: [
      "http://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.2.0.cloudera1.jar"
      # "http://archive.cloudera.com/kudu/csd/KUDU-5.10.2.jar" # in CDH as of 5.13
      "http://archives.streamsets.com/datacollector/2.7.2.0/csd/STREAMSETS-2.7.2.0.jar"
      "http://archive.cloudera.com/cdsw/1/csd/CLOUDERA_DATA_SCIENCE_WORKBENCH-1.2.1.jar"
  ]   

  enableEnterpriseTrial: true

  # Install unlimited strength JCE policy files?
  unlimitedJce: true
  #
  # include file("gg-kerberos-credentials.conf")
  # krbAdminUsername: "cloudera-scm@GREGORYG.COM"
  # krbAdminPassword: "zlort0"

  configs {
    CLOUDERA_MANAGER {
      enable_api_debug: true
      custom_banner_html: "Peace among worlds!"
      CLUSTER_STATS_COUNT: 5
      CLUSTER_STATS_DEFAULT_SIZE_MB: 200
      CLUSTER_STATS_PATH: "/tmp"
      CLUSTER_STATS_SCHEDULE: "DAILY"
}
   SERVICEMONITOR {
      firehose_heapsize: 2147483648
      firehose_non_java_memory_bytes: 12884901888
   }

  }

  databaseTemplates {
    CLOUDERA_MANAGER {
      name: cmtemplate
      databaseServerName: mysqlmeta # Must correspond to an external database server named above
      databaseNamePrefix: scm
      usernamePrefix: cmadmin
    }

    ACTIVITYMONITOR {
      name: cmtemplate
      databaseServerName: mysqlmeta # Must correspond to an external database server named above
      databaseNamePrefix: amon
      usernamePrefix: cmadmin
    }

    REPORTSMANAGER {
      name: cmtemplate
      databaseServerName: mysqlmeta # Must correspond to an external database server named above
      databaseNamePrefix: rptman
      usernamePrefix: cmadmin
    }


    NAVIGATOR {
      name: cmtemplate
      databaseServerName: mysqlmeta # Must correspond to an external database server named above
      databaseNamePrefix: nav
      usernamePrefix: cmadmin
    }

    # Added in Cloudera Manager 5.2+
    NAVIGATORMETASERVER {
      name: cmtemplate
      databaseServerName: mysqlmeta # Must correspond to an external database server named above
      databaseNamePrefix: navmeta
      usernamePrefix: cmadmin
    }
  }


}

#
# Cluster description
#
cluster {
  # Optional override of CDH parcel repositories
  # javaInstallationStrategy: NONE
  products {
    CDH: 5
    KAFKA: 3
    # KUDU: 1
    SPARK2: 2
    # SPARK_ON_YARN: 1
    # Anaconda: 4.2
    # STREAMSETS_DATACOLLECTOR: 2
    # dmexpress: 9
  }
  parcelRepositories: [
    "http://archive.cloudera.com/cdh5/parcels/5.13/"
    "http://archive.cloudera.com/spark2/parcels/2.2/"
    "http://archive.cloudera.com/kafka/parcels/latest/"
    # "http://dynapse.net/archive/dmexpress/"
    # "http://archive.cloudera.com/beta/recordservice/parcels/latest/"
    "http://archives.streamsets.com/datacollector/latest/parcel/"
  ]

  services: [
              HDFS
              YARN
              ZOOKEEPER
              KAFKA
              HIVE
              HUE
              OOZIE
              IMPALA
              # SOLR
              # FLUME
              # SQOOP
              SPARK2_ON_YARN
              KUDU
              # CDSW
              # SENTRY
            ]

  ## NOTE: to get all role types for CM, use the API:
  ##     curl -u 'admin:admin'  http://10.240.0.3:7180/api/v14/clusters/lemoncheeks/services  ## to get the name of the service
  ##     curl -u 'admin:admin'  http://10.240.0.3:7180/api/v14/clusters/lemoncheeks/services/CD-IMPALA-CWufpYcy/roleTypes


  # S3 Configurations
  configs {
    HDFS {
      core_site_safety_valve: """
            <property>
                <name>fs.s3a.access.key</name>
                <value>"""${AWS_ACCESS_KEY_ID}"""</value>
            </property>
            <property>
                <name>fs.s3a.secret.key</name>
                <value>"""${AWS_SECRET_ACCESS_KEY}"""</value>
            </property>
        """
    }
    HUE {
      hue_service_safety_valve: """[notebook]
show_notebooks=true"""
      }

  }

  databaseTemplates: {
    HIVE {
      name: hivetemplate
      databaseServerName: mysqlmeta # Must correspond to an external database server named above
      databaseNamePrefix: hivemeta
      usernamePrefix: hive
    },
    # SENTRY {
    #   name: sentrytemplate
    #   databaseServerName: mysqlmeta # Must correspond to an external database server named above
    #   databaseNamePrefix: sentry
    #   usernamePrefix: sentry
    # },
    HUE {
      name: huetemplate
      databaseServerName: mysqlmeta # Must correspond to an external database server named above
      databaseNamePrefix: hue
      usernamePrefix: hue
    },
    OOZIE {
      name: oozietemplate
      databaseServerName: mysqlmeta # Must correspond to an external database server named above
      databaseNamePrefix: oozie
      usernamePrefix: oozie
    }
    # SQOOP {
    #   name: sqooptemplate
    #   databaseServerName: mysqlmeta # Must correspond to an external database server named above
    #   databaseNamePrefix: sqoop
    #   usernamePrefix: sqoop
    # }
  }

  devgateways {
    # for simple development environment - can use with AWS Cloud9
    count: 1
    instance: ${instances.m42x} {
      instanceNamePrefix: dev
      tags {
        group: devgateway
      }
      bootstrapScriptsPaths: ["java8-bootstrap.sh", "prep-cloud9-dev-node.sh"]
  }
      roles {
      HDFS: [GATEWAY]
      HIVE: [GATEWAY]
      KAFKA: [GATEWAY]
      # SOLR: [GATEWAY]
      # SPARK_ON_YARN: [GATEWAY]
      SPARK2_ON_YARN: [GATEWAY]
      YARN: [GATEWAY]
      # SENTRY: [GATEWAY]
    }
    }

  gateways {
    # Includes disks needed by CDSW
    count: 1
    minCount: 1
    instance: ${instances.m42x} {
      instanceNamePrefix: gw
      tags {
        group: gateway
      }
      # These could be added to cloud specific config files but included here to keep in sync
      # Disks for gcp and azure
      bootDiskSizeGb: 100
      dataDiskCount: 2
      dataDiskSizeGb: 600
      dataDiskType: Standard

      # Disks for aws
      rootVolumeSizeGB: 100
      ebsVolumeCount : 2
      ebsVolumeSizeGiB: 600
      ebsVolumeType: gp2
      bootstrapScriptsPaths: ["java8-bootstrap.sh", "prep-cdsw-master-node.sh"]
    }
    roles {
      # FLUME: [AGENT]
      # HBASE: [GATEWAY]
      HDFS: [GATEWAY]
      HIVE: [GATEWAY]
      KAFKA: [GATEWAY]
      # SOLR: [GATEWAY]
      # SPARK_ON_YARN: [GATEWAY]
      SPARK2_ON_YARN: [GATEWAY]
      YARN: [GATEWAY]
      # SENTRY: [GATEWAY]
    }
      configs {
      SPARK2_ON_YARN {
          GATEWAY {
              "spark2-conf/spark-env.sh_client_config_safety_valve": """if [ -z "${PYSPARK_PYTHON}" ]; then export PYSPARK_PYTHON=/opt/cloudera/parcels/Anaconda/bin/python; fi"""
                }
            }
        }
  }

  masters {
    count: 1
    instance: ${instances.m42x} {
      instanceNamePrefix: master
      # placementGroup: praseed-pg
      tags {
        group: master
      }

    }
    roles {
      HDFS: [NAMENODE, SECONDARYNAMENODE, BALANCER]
      HIVE: [HIVESERVER2, HIVEMETASTORE]
      HUE: [HUE_SERVER, HUE_LOAD_BALANCER]
      IMPALA: [STATESTORE, CATALOGSERVER]
      KUDU: [KUDU_MASTER]
      OOZIE: [OOZIE_SERVER]
      # SOLR: [SOLR_SERVER]
      YARN: [RESOURCEMANAGER, JOBHISTORY]
      ZOOKEEPER: [SERVER]
      # HBASE: [MASTER, HBASETHRIFTSERVER]
      # HIVE: [HIVESERVER2, HIVEMETASTORE, WEBHCAT]
      # SPARK_ON_YARN: [SPARK_YARN_HISTORY_SERVER, GATEWAY]
      SPARK2_ON_YARN: [SPARK2_YARN_HISTORY_SERVER, GATEWAY]
      KAFKA: [KAFKA_BROKER]
      # KS_INDEXER: [HBASE_INDEXER]
      # SENTRY: [SENTRY_SERVER]
      # SQOOP: [SQOOP_SERVER]
    }
    # Optional custom role configurations
    # Configuration keys containing special characters (e.g., '.', ':') must be enclosed in double quotes.
    #
    configs {
      HDFS {
        NAMENODE {
          namenode_java_heapsize: 4294967296
          #        dfs_name_dir_list: /data/nn
          #        namenode_port: 1234
        }
        SECONDARYNAMENODE {
          secondary_namenode_java_heapsize: 4294967296
        }
      }
      HIVE {
        HIVESERVER2 {
          hiveserver2_spark_executor_cores: 4
        }
      }
      KUDU {
        KUDU_MASTER {
          fs_wal_dir: "/data0/kudu/masterwal"
          fs_data_dirs: "/data1/kudu/master"
        }
      }
      # AWS_S3 {
      #        cloud_account_name: Gorto
      #        fs.s3a.endpoint: "s3.amazonaws.com"
      # },
      KAFKA {
        KAFKA_BROKER {
          broker_max_heap_size: 512
          # "log.dirs": /data0/kafka/data
        }
      }
    }
  }

  zoo-workers {
    count: 2
    minCount: 2
    instance: ${instances.m42x-disks} {
      instanceNamePrefix: zwork
      # placementGroup: praseed-pg
      tags {
        group: worker
      }
    }
    roles {
      HDFS: [DATANODE]
      YARN: [NODEMANAGER]
      IMPALA: [IMPALAD]
      ZOOKEEPER: [SERVER]
      # SPARK_ON_YARN: [GATEWAY]
      SPARK2_ON_YARN: [GATEWAY]
      # HBASE: [REGIONSERVER]
      # FLUME: [AGENT]
      KUDU: [KUDU_TSERVER]
    }
    configs {
      KUDU {
        KUDU_TSERVER {
          fs_wal_dir: "/data2/kudu/tabletwal"
          fs_data_dirs: "/data3/kudu/tablet"
        }
      }
    }
  }

  workers {
    count: 1
    minCount: 1
    instance: ${instances.m42x-disks} {
      instanceNamePrefix: work
      # placementGroup: praseed-pg
      tags {
        group: worker
      }
    }
    roles {
      HDFS: [DATANODE]
      YARN: [NODEMANAGER]
      IMPALA: [IMPALAD]
      # SPARK_ON_YARN: [GATEWAY]
      SPARK2_ON_YARN: [GATEWAY]
      # HBASE: [REGIONSERVER]
      # FLUME: [AGENT]
      KUDU: [KUDU_TSERVER]
    }
    configs {
      KUDU {
        KUDU_TSERVER {
          fs_wal_dir: "/data2/kudu/tabletwal"
          fs_data_dirs: "/data3/kudu/tablet"
        }
      }
    }
  }

  # task-runner {
  #   count: 0
  #   instanceNamePrefix: task
  #   instance: ${instances.m42x} {
  #     tags {group: taskrunner}
  #   }
  # }

  postCreateScripts: ["""#!/bin/sh
sudo -u hdfs hdfs dfs -mkdir /user/"""${ssh.username}"""
sudo -u hdfs hdfs dfs -chown """${ssh.username}""":"""${ssh.username}""" /user/"""${ssh.username}"""
    """
    ]

  instancePostCreateScriptsPaths: ["cdsw-post.sh"]
  # instancePostCreateScriptPaths: ["cdsw-post.sh"]

#   ## one can use a combination of postCreateScripts and postCreateScriptPaths ; scripts are run on one random host
#   postCreateScripts: ["""#!/bin/bash
# yum -y install epel-release
# yum -y install python-pip wget curl jq htop mlocate finger
# pip install cm-api
# """,
# """#!/usr/bin/python
# import os
# import time
# from cm_api.api_client import ApiResource
# # If the exit code is not zero Cloudera Director will fail

# # Post creation scripts also have access to the following environment variables:

# #    DEPLOYMENT_HOST_PORT
# #    ENVIRONMENT_NAME
# #    DEPLOYMENT_NAME
# #    CLUSTER_NAME
# #    CM_USERNAME
# #    CM_PASSWORD

# # echo "TODO: Installing Anaconda python parcel"
# # echo "Add repository"

# def main():
#     anaconda_repo="https://repo.continuum.io/pkgs/misc/parcels/"
#     cmhost = os.environ['DEPLOYMENT_HOST_PORT'].split(":")[0]
#     api = ApiResource(cmhost, username='admin', password='admin')
#     all_clusters = api.get_all_clusters()
#     for cluster in all_clusters:
#         if (cluster.name == os.environ['CLUSTER_NAME']):
#             break

#     cm = api.get_cloudera_manager()
#     config = cm.get_config(view='summary')
#     parcel_urls = config.get("REMOTE_PARCEL_REPO_URLS","").split(",")
#     print "Adding Anaconda parcel repository"
#     if anaconda_repo in parcel_urls:
#         # already there, skip
#         print "Anaconda repo url already included"
#     else:
#         parcel_urls.append(anaconda_repo)
#         config["REMOTE_PARCEL_REPO_URLS"] = ",".join(parcel_urls)
#         cm.update_config(config)
#         # wait to make sure parcels are refreshed
#         time.sleep(10)
#         print "Added Anaconda repository URL"

#     # Download Anaconda parcel
#     print "Downloading parcel"
#     parcel = cluster.get_parcel('Anaconda', '4.1.1')
#     parcel.start_download()
#     while True:
#         parcel = cluster.get_parcel('Anaconda', '4.1.1')
#         if parcel.stage == 'DOWNLOADED':
#             break
#         if parcel.state.errors:
#             raise Exception(str(parcel.state.errors))
#         print "   progress: %s of %s" % (parcel.state.progress, parcel.state.totalProgress)
#         time.sleep(2)
#     print "Downloaded"

#     print "Distributing"
#     parcel.start_distribution()
#     while True:
#         parcel = cluster.get_parcel('Anaconda', '4.1.1')
#         if parcel.stage == 'DISTRIBUTED':
#             break
#         if parcel.state.errors:
#             raise Exception(str(parcel.state.errors))
#         print "   progress: %s of %s" % (parcel.state.progress, parcel.state.totalProgress)
#         time.sleep(5)
#     print "Distributed"

#     print "Activating Anaconda"
#     parcel.activate()
#     while True:
#         parcel = cluster.get_parcel('Anaconda', '4.1.1')
#         if parcel.stage == 'ACTIVATED':
#             break
#         time.sleep(2)
#     print "Activated. Anaconda is ready to use"

    
# if __name__ == "__main__":
#     main()

#     """]

#   # For more complex scripts, post creation scripts can be supplied via path,
#   # where they will be read from the local filesystem.  They will run after
#   # any scripts supplied in the previous postCreateScripts section.
#   # postCreateScriptsPaths: ["/tmp/test-script.sh",
#   #                         "/tmp/test-script.py"]

#   preTerminateScripts: ["""#!/bin/sh

# # This is an embedded pre-termination script that runs as root and can be used to
# # customize the cluster after it has been created.

# # If the exit code is not zero Cloudera Director will fail

# # Pre terminate scripts also have access to the following environment variables:

# #    DEPLOYMENT_HOST_PORT
# #    ENVIRONMENT_NAME
# #    DEPLOYMENT_NAME
# #    CLUSTER_NAME
# #    CM_USERNAME
# #    CM_PASSWORD

# echo 'Goodbye World!'
# exit 0
#     """,
#     """#!/usr/bin/python

# # Additionally, multiple pre terminate scripts can be supplied.  They will run
# # in the order they are listed here.  Interpeters other than bash can be used
# # as well.

# print 'Goodbye again!'
#         """]

#   # For more complex scripts, pre terminate scripts can be supplied via path,
#   # where they will be read from the local filesystem.  They will run after
#   # any scripts supplied in the previous preTerminateScripts section.
#   # preTerminateScriptsPaths: ["/tmp/test-script.sh",
#   #                            "/tmp/test-script.py"]
}
